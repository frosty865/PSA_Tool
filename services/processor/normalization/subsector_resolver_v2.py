"""
subsector_resolver_v2.py

Document-level subsector resolver for VOFC Engine.

Uses:
  - keywords
  - phrases
  - synonyms
  - semantic similarity (optional, SentenceTransformers)
  - sector locking / boosting

Input:  document text (title + first pages + metadata)
Output: best subsector (id, name, sector_id, score) + optional candidate list

Designed to work with `subsector_vocabulary.json` generated by
`generate_subsector_vocabulary.py`.
"""

from __future__ import annotations

import json
import re
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional

# Optional semantic support
try:
    from sentence_transformers import SentenceTransformer, util
    _HAS_ST = True
except ImportError:
    _HAS_ST = False

logger = logging.getLogger(__name__)

# -------------------------------------------------------------------
# Basic text utilities
# -------------------------------------------------------------------

def _normalize(text: str) -> str:
    if not text:
        return ""
    t = text.lower()
    t = re.sub(r"[^a-z0-9\s]", " ", t)
    t = re.sub(r"\s+", " ", t)
    return t.strip()


def _tokenize(text: str) -> List[str]:
    text = _normalize(text)
    if not text:
        return []
    return text.split()


def _ngrams(tokens: List[str], n: int) -> List[str]:
    return [" ".join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]


# -------------------------------------------------------------------
# Data structures
# -------------------------------------------------------------------

@dataclass
class SubsectorEntry:
    id: str
    name: str
    sector_id: str
    keywords: List[str] = field(default_factory=list)
    phrases: List[str] = field(default_factory=list)
    synonyms: List[str] = field(default_factory=list)
    semantic_seeds: List[str] = field(default_factory=list)


@dataclass
class SubsectorScore:
    id: str
    name: str
    sector_id: str
    total_score: float
    keyword_hits: int
    phrase_hits: int
    synonym_hits: int
    semantic_score: float
    details: Dict[str, Any] = field(default_factory=dict)


# -------------------------------------------------------------------
# Resolver
# -------------------------------------------------------------------

class SubsectorResolverV2:
    """
    Document-level subsector resolver.

    Usage:
        resolver = SubsectorResolverV2("subsector_vocabulary.json")
        result = resolver.resolve_document(
            text=document_text,
            known_sector_id=None,
            top_k=5,
            return_debug=True,
        )
    """

    def __init__(
        self,
        vocab_source: str | Path | Dict[str, Any],
        model_name: str = "all-MiniLM-L6-v2",
        enable_semantic: bool = True,
    ) -> None:
        """
        vocab_source:
            - path to subsector_vocabulary.json
            - or dict already loaded from JSON
        """
        if isinstance(vocab_source, (str, Path)):
            vocab_path = Path(vocab_source)
            if not vocab_path.exists():
                raise FileNotFoundError(f"Subsector vocab not found: {vocab_path}")
            with vocab_path.open("r", encoding="utf-8") as f:
                vocab = json.load(f)
        elif isinstance(vocab_source, dict):
            vocab = vocab_source
        else:
            raise TypeError("vocab_source must be path or dict")

        self.subsectors: List[SubsectorEntry] = []
        for row in vocab.get("subsectors", []):
            self.subsectors.append(
                SubsectorEntry(
                    id=str(row["id"]),
                    name=row["name"],
                    sector_id=row["sector_id"],
                    keywords=row.get("keywords", []),
                    phrases=row.get("phrases", []),
                    synonyms=row.get("synonyms", []),
                    semantic_seeds=row.get("semantic_seeds", []),
                )
            )

        self.enable_semantic = enable_semantic and _HAS_ST
        self.model: Optional[SentenceTransformer] = None
        self._embeddings: Dict[str, Any] = {}

        if self.enable_semantic:
            self._init_semantic_model(model_name)

    # -------------------------------------------------------------------
    # Semantic model
    # -------------------------------------------------------------------

    def _init_semantic_model(self, model_name: str) -> None:
        try:
            self.model = SentenceTransformer(model_name)
            for sub in self.subsectors:
                if not sub.semantic_seeds:
                    continue
                emb = self.model.encode(sub.semantic_seeds, convert_to_tensor=True)
                self._embeddings[sub.id] = emb
        except Exception as e:
            logger.warning(f"Failed to load semantic model '{model_name}': {e}. Semantic scoring disabled.")
            self.enable_semantic = False
            self.model = None

    def _semantic_score(self, text: str, subsector: SubsectorEntry) -> float:
        if not self.enable_semantic or not self.model:
            return 0.0
        emb = self._embeddings.get(subsector.id)
        if emb is None:
            return 0.0
        text = (text or "").strip()
        if not text:
            return 0.0
        try:
            t_emb = self.model.encode(text, convert_to_tensor=True)
            sim = util.cos_sim(t_emb, emb)[0]
            return float(sim.max().item())
        except Exception as e:
            logger.debug(f"Semantic scoring failed for '{subsector.name}': {e}")
            return 0.0

    # -------------------------------------------------------------------
    # Scoring internals
    # -------------------------------------------------------------------

    def _score_subsector(
        self,
        text_norm: str,
        tokens: List[str],
        subsector: SubsectorEntry,
        sector_hint: Optional[str] = None,
        semantic_weight: float = 6.0,
    ) -> SubsectorScore:
        grams = set(tokens)
        for n in range(2, 5):
            grams.update(_ngrams(tokens, n))

        score = 0.0
        kw_hits = 0
        ph_hits = 0
        syn_hits = 0

        # Keywords
        for kw in subsector.keywords:
            if not kw:
                continue
            if kw in text_norm or kw in grams:
                score += 2.0
                kw_hits += 1

        # Phrases
        for ph in subsector.phrases:
            if not ph:
                continue
            if ph in text_norm:
                score += 4.0
                ph_hits += 1

        # Synonyms (stronger weight)
        for syn in subsector.synonyms:
            if not syn:
                continue
            syn_norm = _normalize(syn)
            if syn_norm and syn_norm in text_norm:
                score += 5.0
                syn_hits += 1

        # Semantic similarity
        sem_score = self._semantic_score(text_norm, subsector)
        score += semantic_weight * sem_score

        # Sector hint boosting (document-level metadata)
        if sector_hint and subsector.sector_id == sector_hint:
            score *= 1.30  # +30% if matches known sector

        # Very basic relevance floor: if we have no hits at all and semantic is tiny, keep score tiny
        if kw_hits == 0 and ph_hits == 0 and syn_hits == 0 and sem_score < 0.15:
            score *= 0.1

        return SubsectorScore(
            id=subsector.id,
            name=subsector.name,
            sector_id=subsector.sector_id,
            total_score=score,
            keyword_hits=kw_hits,
            phrase_hits=ph_hits,
            synonym_hits=syn_hits,
            semantic_score=sem_score,
            details={
                "keyword_score": kw_hits * 2.0,
                "phrase_score": ph_hits * 4.0,
                "synonym_score": syn_hits * 5.0,
                "semantic_weight": semantic_weight,
            },
        )

    # -------------------------------------------------------------------
    # Public API
    # -------------------------------------------------------------------

    def resolve_document(
        self,
        text: str,
        known_sector_id: Optional[str] = None,
        top_k: int = 3,
        min_score: float = 4.0,
        return_debug: bool = False,
    ) -> Dict[str, Any]:
        """
        Resolve the most likely subsector for a document.

        Parameters
        ----------
        text : str
            Document-level text (title, intro, facility description, etc.)
        known_sector_id : Optional[str]
            If you already know the sector (e.g., from metadata), pass it here
            to boost matching subsectors.
        top_k : int
            Number of candidates to include in debug output.
        min_score : float
            Minimum score to accept a subsector as "confident".
        return_debug : bool
            If True, returns full candidate list and scoring details.

        Returns
        -------
        dict:
            {
              "subsector_id": str or None,
              "subsector_name": str or None,
              "sector_id": str or None,
              "score": float,
              "confidence": float,        # 0â€“1 estimate,
              "raw_text": "...",          # if debug
              "candidates": [ ... ]       # if debug
            }
        """
        text = text or ""
        text_norm = _normalize(text)
        tokens = _tokenize(text)

        if not text_norm or not tokens:
            result = {
                "subsector_id": None,
                "subsector_name": None,
                "sector_id": None,
                "score": 0.0,
                "confidence": 0.0,
            }
            if return_debug:
                result["raw_text"] = text
                result["candidates"] = []
            return result

        scores: List[SubsectorScore] = []
        for subsector in self.subsectors:
            s = self._score_subsector(
                text_norm=text_norm,
                tokens=tokens,
                subsector=subsector,
                sector_hint=known_sector_id,
            )
            scores.append(s)

        scores.sort(key=lambda s: s.total_score, reverse=True)
        best = scores[0] if scores else None

        if not best or best.total_score < min_score:
            result = {
                "subsector_id": None,
                "subsector_name": None,
                "sector_id": None,
                "score": best.total_score if best else 0.0,
                "confidence": 0.0,
            }
            if return_debug:
                result["raw_text"] = text
                result["candidates"] = [
                    _score_to_dict(s) for s in scores[:top_k]
                ]
            return result

        # crude confidence: best / (best + second_best + 1)
        if len(scores) > 1:
            denom = best.total_score + scores[1].total_score + 1e-6
            confidence = best.total_score / denom
        else:
            confidence = 1.0

        result = {
            "subsector_id": best.id,
            "subsector_name": best.name,
            "sector_id": best.sector_id,
            "score": best.total_score,
            "confidence": float(round(confidence, 3)),
        }

        if return_debug:
            result["raw_text"] = text
            result["candidates"] = [
                _score_to_dict(s) for s in scores[:top_k]
            ]

        return result


def _score_to_dict(s: SubsectorScore) -> Dict[str, Any]:
    return {
        "subsector_id": s.id,
        "subsector_name": s.name,
        "sector_id": s.sector_id,
        "score": s.total_score,
        "keyword_hits": s.keyword_hits,
        "phrase_hits": s.phrase_hits,
        "synonym_hits": s.synonym_hits,
        "semantic_score": s.semantic_score,
        "details": s.details,
    }

